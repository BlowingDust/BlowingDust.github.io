<!DOCTYPE html>
<html>
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="referrer" content="no-referrer">
        <title>Dust's Blog - Articles by Dust</title>
        <meta charset="utf-8" />

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/muicss@0.9.41/dist/css/mui.min.css"
              integrity="sha256-oX1CoieDE574GYL7nKAFSFOt8BdQzeKvATYS4VqFqNw=" crossorigin="anonymous">
        <link href="/theme/css/main.css" rel="stylesheet">
        <link href="/theme/css/highlight.css" rel="stylesheet">
</head>

<body id="index" class="home">
  <div id="container" class="mui-container">
        <header id="banner" class="body">
                <h1><a href="/">Dust's Blog <strong></strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li>
              <div class="index-cat"><a href="/">Index</a></div>
            </li>
              <li>
                <div  class="last-cat" >
                  <a href="/category/tech.html">Tech</a>
                </div>
              </li>
        </ul></nav><!-- /#menu -->
<section id="content" class="mui--divider-top">
<h2>Articles by Dust</h2>

<ol id="post-list">
        <li><article class="hentry ">
                <header> <h2 class="entry-title"><a href="/PVE-configuration-for-AMD-GPU-passthrough.html" rel="bookmark">PVE 配置 AMD 显卡直通</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <time class="published" datetime="2025-05-16T16:34:00+08:00">
      🖊️ 2025-05-16 16:34:00
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>本篇文章主要是 PVE 配置 AMD 显卡直通的教程。原本以为要花几天时间折腾，结果只花了一个晚上就配置完成，没有想象中的复杂。大部分教程内容由 ChatGPT 编写，我根据实际情况和遇到的问题，作一些改动和补充说明。</p>
<hr>
<h2>🎯 教程目标</h2>
<p>将 AMD 独立显卡通过 IOMMU 技术直通给 Proxmox VE（PVE）中的 Windows 虚拟机，从而在虚拟机中获得接近原生的图形性能。</p>
<hr>
<h2>✅ 教程前提条件</h2>
<h3>硬件要求：</h3>
<ul>
<li>支持虚拟化的 CPU（支持 AMD-V 或 Intel VT-d）</li>
<li>支持 IOMMU 的主板（开启 BIOS 中的 IOMMU 或 VT-d）</li>
<li>AMD 独立显卡</li>
</ul>
<h3>软件环境：</h3>
<ul>
<li>Proxmox VE 7.x 或 8.x</li>
<li>Windows 10/11 虚拟机</li>
</ul>
<hr>
<p><strong>【补充】</strong></p>
<p>我的实际环境如下 …</p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/PVE-configuration-for-AMD-GPU-passthrough.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/2024-year-end-summary.html" rel="bookmark">2024 年终总结</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <time class="published" datetime="2025-01-26T14:10:00+08:00">
      🖊️ 2025-01-26 14:10:00
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>本来在今年年初休假期间就写好了年终总结，不过并不想公开发布，一是文中有些敏感的项目信息，二是找不到发布的理由，不做不必要之事。不过后来想到万一有人好奇来问我你在日本都做了些什么，我便可把此文链接发给他。因此相比原文做了一些删减和加笔。</p>
<h3>A1 项目</h3>
<p>2023年12月入场，到2024年2月时结束，总计3个月的A1项目工作。虽是初入日本现场，但并不想久留。项目历经多年而技术老旧，立项时间似乎可追溯到本世纪初。虽有文档但不知所云，毕竟是银行外汇、电汇相关的SWIFT电文，专业性相当强，感觉换成中文都不一定能看懂。</p>
<p>其中 …</p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/2024-year-end-summary.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/font-code-point.html" rel="bookmark">基于字体字符码点的文字异常检测</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/python.html">python</a>    </p>
    <time class="published" datetime="2023-02-24T11:12:00+08:00">
      🖊️ 2023-02-24 11:12:00
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <h2>0</h2>
<p>本文以《新世纪日汉双解大辞典》为例，介绍一种辞典文本错字的检测方法。
<br></p>
<h2>1</h2>
<p>近日找寻日文辞典，下载《新世纪日汉双解大辞典》后，自行改写CSS样式，变更日文字体为教科书体（UD デジタル 教科書体 N-R，Windows 10 自带字体），然而后续检查中发现个别文字字体样式有异，如图中「边」字。</p>
<p><img alt="font fallback" src="images/font_fallback.jpg"></p>
<p>原因何在？首先需要了解字体显示的回退机制（fallback）：在网页中，如果某个字符在CSS指定的字体中不存在，那么排版引擎将根据内置的预设字体作为后备显示方案。</p>
<p>此处，CSS样式指定的日文字体仅包含日文汉字，无法显示作为中文简体的「边」字，由于回退机制，排版引擎便以衬线字体展示，而 …</p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/font-code-point.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/windows-rsync-backup.html" rel="bookmark">在 Windows 中使用 rsync 备份文件</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/rsync.html">rsync</a>    </p>
    <time class="published" datetime="2021-01-24T11:11:00+08:00">
      🖊️ 2021-01-24 11:11:00
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>笔记本使用的是固态硬盘，有掉盘风险，尽管已用 OneDrive 备份了重要文件，但仍有很多文件属于“虽然没那么重要，但如果丢失了也很心痛”的情况。于是决定使用移动固态硬盘做冷备份。</p>
<p>在 Windows 中，rsync 可通过 Cygwin 安装。</p>
<p>执行以下命令可进行测试备份：</p>
<div class="highlight"><pre><span></span><code>rsync -gloptrn --delete $srcdir $dstdir
</code></pre></div>

<p>参数解析：</p>
<div class="highlight"><pre><span></span><code>g - 保留所属组信息
l - 保留软链接
o - 保留所属人信息
p - 保留权限信息
t - 保留修改时间
r - 递归子目录
n - 测试执行，不进行实际文件复制操作
delete - 删除目标目录（$dstdir）下无关的文件
</code></pre></div>

<p>建议开始时先使用文件数较少的目录进行测试，确认无 …</p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/windows-rsync-backup.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/raspberry-downloader.html" rel="bookmark">利用树莓派进行挂机下载</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/raspberry.html">raspberry</a>    </p>
    <time class="published" datetime="2019-09-07T18:59:00+08:00">
      🖊️ 2019-09-07 18:59:00
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>一直有买 NAS 做下载姬的想法，但见过太多人买回家吃灰的例子，而且我的松鼠症并不严重，所以犹豫着。直到听闻树莓派4代发售，价格比 NAS 便宜，正好家里也有闲置 U 盘，思来想去，果然还是树莓派比较适合老子.jpg</p>
<h2>1. 准备工作</h2>
<p>在淘宝找到相关店家，根据自己的需要选择相应套餐，因为需要预订，大概一周后到货。</p>
<p>树莓派到手后，先将 Raspbian 系统镜像写入 micro SD 卡，推荐使用 <a href="https://www.raspberrypi.org/documentation/installation/installing-images/README.md">官方教程</a> 提到的 balenaEtcher，十分方便。</p>
<p><img alt="balenaEtcher" src="/images/etchcer.jpg"></p>
<p>之后配置远程 SSH 和 wifi（如果选择直接在树莓派上外接显示器和键盘就不用进行这一步），简单来说就是 …</p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/raspberry-downloader.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/phpMyAdmin-docker-config.html" rel="bookmark">phpMyAdmin Docker 简单配置</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/docker.html">docker</a>,            <a href="/tag/phpmyadmin.html">phpMyAdmin</a>    </p>
    <time class="published" datetime="2019-06-21T16:43:41+08:00">
      🖊️ 2019-06-21 16:43:41
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>phpMyAdmin（以下简称 PMA ）的数据字典（Data Dictionary）对我来说是一个比较有用的功能（实际上我只用这个功能...），可以以网页形式浏览所有表、视图、字段、索引、注释等，在一个有数百张表的数据库中可以快速检索所需信息。</p>
<p><img alt="image" src="/images/pma_data_dict.png"></p>
<p>利用 Docker 可以方便快速地在本地部署 PMA。</p>
<p>下载</p>
<p><code>docker pull phpmyadmin/phpmyadmin</code></p>
<p>运行</p>
<p><code>docker run --name PMA -d -p 8080:80 --restart=always -e PMA_ARBITRARY=1 -v /path/to/phpmyadmin/config.user.inc.php:/etc/phpmyadmin/config.user.inc.php -v /path/to/phpmyadmin/php.ini:/usr/local/etc/php/conf.d/php.ini phpmyadmin/phpmyadmin</code></p>
<p><code>-e PMA_ARBITRARY=1</code>  表示默认可连接任意数据库。</p>
<p>同时我挂载了两个文件到容器中，一个是 PMA 的配置文件，另一个是 PHP 的配置文件。原因是 PMA …</p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/phpMyAdmin-docker-config.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/encrypted-compression-javascript-analysis.html" rel="bookmark">数据抓取实践：对加密参数及压缩混淆JS的逆向分析</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/python.html">python</a>,            <a href="/tag/frontend.html">frontend</a>,            <a href="/tag/crawler.html">crawler</a>    </p>
    <time class="published" datetime="2018-05-15T18:40:27+08:00">
      🖊️ 2018-05-15 18:40:27
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>之前在 V 站上看到一个帖子，内容是楼主想抓取数据的网站做了参数加密，不知如何解密。刚好最近有点空闲时间，可以尝试解密一下网页参数，而且很久没练手了。</p>
<p>文中会介绍几种分析技巧，需要一点前端知识（总感觉在前端做防爬没什么意义，因为源码都是公开的）。文末附上爬虫 Demo 验证，虽然对于这个案例来说使用 Selenium 可能才是合适的解决方法，但<del>暴力破解才是男人的浪漫！</del> ...嗯本文的重点只是在于分析解密的过程。有些图片因代码过长未包含在内，意会即可。</p>
<h2>1. 一夫当关 - XHR Breakpoints</h2>
<p>网站是<a href="https://www.qimai.cn/rank/">七 …</a></p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/encrypted-compression-javascript-analysis.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/scrapy-start-requests-trap.html" rel="bookmark">Scrapy 暗坑之 start_requests</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/python.html">python</a>,            <a href="/tag/scrapy.html">scrapy</a>    </p>
    <time class="published" datetime="2018-03-01T14:03:42+08:00">
      🖊️ 2018-03-01 14:03:42
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>众所周知，Scrapy 默认会过滤重复的 URL，不会重复抓取相同的 URL，除非显式指定。</p>
<p>于是随便写了一个爬图片地址的小虫，然而不知道为什么总会爬两次 baidu 首页，你能看出错在哪里吗？</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ImageSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;images&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;www.baidu.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://www.baidu.com/&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
            <span class="n">image_item</span> <span class="o">=</span> <span class="n">ImageItem</span><span class="p">()</span>
            <span class="n">image_item</span><span class="p">[</span><span class="s1">&#39;img_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
            <span class="k">yield</span> <span class="n">image_item</span>

        <span class="n">urls</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">url</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">next_url</span><span class="p">)</span>
</code></pre></div>

<p>我想了半天都不明白为什么，以为是过滤器的问题，查了半天资料仍没解决。 后来偶然看了 Spider 源码，才发现坑爹之处。</p>
<p>原来源码的 start_requests 是这样写的（已忽略无关代码）</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self …</span></code></pre></div> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/scrapy-start-requests-trap.html">>> Read More</a>
                </div>
        </article></li>
</ol><!-- /#posts-list -->
</section><!-- /#content -->

        <footer id="contentinfo" class="body mui--divider-top">
          <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh">
            <img id="cc" src="/images/cc.80x15.png" alt="CC BY-SA"></a>
                <address id="about" class="vcard body">
                Powered by <a href="https://github.com/getpelican/pelican">Pelican</a>.
                Theme <a href="/">Autism</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
  </div>

  <script defer src="/theme/js/image.js"></script>
</body>
</html>