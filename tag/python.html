<!DOCTYPE html>
<html>
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="referrer" content="no-referrer">
        <title>Dust's Blog - python</title>
        <meta charset="utf-8" />

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/muicss@0.9.41/dist/css/mui.min.css"
              integrity="sha256-oX1CoieDE574GYL7nKAFSFOt8BdQzeKvATYS4VqFqNw=" crossorigin="anonymous">
        <link href="/theme/css/main.css" rel="stylesheet">
        <link href="/theme/css/highlight.css" rel="stylesheet">
</head>

<body id="index" class="home">
  <div id="container" class="mui-container">
        <header id="banner" class="body">
                <h1><a href="/">Dust's Blog <strong></strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li>
              <div class="index-cat"><a href="/">Index</a></div>
            </li>
              <li>
                <div  class="last-cat" >
                  <a href="/category/tech.html">Tech</a>
                </div>
              </li>
        </ul></nav><!-- /#menu -->
<section id="content" class="mui--divider-top">
<h2 class="mui--text-center">Tag - python</h2>

<ol id="post-list">
        <li><article class="hentry ">
                <header> <h2 class="entry-title"><a href="/font-code-point.html" rel="bookmark">基于字体字符码点的文字异常检测</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/python.html">python</a>    </p>
    <time class="published" datetime="2023-02-24T11:12:00+08:00">
      🖊️ 2023-02-24 11:12:00
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <h2>0</h2>
<p>本文以《新世纪日汉双解大辞典》为例，介绍一种辞典文本错字的检测方法。
<br></p>
<h2>1</h2>
<p>近日找寻日文辞典，下载《新世纪日汉双解大辞典》后，自行改写CSS样式，变更日文字体为教科书体（UD デジタル 教科書体 N-R，Windows 10 自带字体），然而后续检查中发现个别文字字体样式有异，如图中「边」字。</p>
<p><img alt="font fallback" src="images/font_fallback.jpg"></p>
<p>原因何在？首先需要了解字体显示的回退机制（fallback）：在网页中，如果某个字符在CSS指定的字体中不存在，那么排版引擎将根据内置的预设字体作为后备显示方案。</p>
<p>此处，CSS样式指定的日文字体仅包含日文汉字，无法显示作为中文简体的「边」字，由于回退机制，排版引擎便以衬线字体展示，而 …</p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/font-code-point.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/encrypted-compression-javascript-analysis.html" rel="bookmark">数据抓取实践：对加密参数及压缩混淆JS的逆向分析</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/python.html">python</a>,            <a href="/tag/frontend.html">frontend</a>,            <a href="/tag/crawler.html">crawler</a>    </p>
    <time class="published" datetime="2018-05-15T18:40:27+08:00">
      🖊️ 2018-05-15 18:40:27
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>之前在 V 站上看到一个帖子，内容是楼主想抓取数据的网站做了参数加密，不知如何解密。刚好最近有点空闲时间，可以尝试解密一下网页参数，而且很久没练手了。</p>
<p>文中会介绍几种分析技巧，需要一点前端知识（总感觉在前端做防爬没什么意义，因为源码都是公开的）。文末附上爬虫 Demo 验证，虽然对于这个案例来说使用 Selenium 可能才是合适的解决方法，但<del>暴力破解才是男人的浪漫！</del> ...嗯本文的重点只是在于分析解密的过程。有些图片因代码过长未包含在内，意会即可。</p>
<h2>1. 一夫当关 - XHR Breakpoints</h2>
<p>网站是<a href="https://www.qimai.cn/rank/">七 …</a></p> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/encrypted-compression-javascript-analysis.html">>> Read More</a>
                </div>
        </article></li>
        <li><article class="hentry mui--divider-top ">
                <header> <h2 class="entry-title"><a href="/scrapy-start-requests-trap.html" rel="bookmark">Scrapy 暗坑之 start_requests</a></h2> </header>
<footer class="post-info">
    <div class="category-info">
        📁 <a href="/category/tech.html">Tech</a>
    </div>
    <p class="tag-info">
 📌             <a href="/tag/python.html">python</a>,            <a href="/tag/scrapy.html">scrapy</a>    </p>
    <time class="published" datetime="2018-03-01T14:03:42+08:00">
      🖊️ 2018-03-01 14:03:42
    </time>
  </footer><!-- /.post-info -->                <div class="entry-content"> <p>众所周知，Scrapy 默认会过滤重复的 URL，不会重复抓取相同的 URL，除非显式指定。</p>
<p>于是随便写了一个爬图片地址的小虫，然而不知道为什么总会爬两次 baidu 首页，你能看出错在哪里吗？</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ImageSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;images&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;www.baidu.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://www.baidu.com/&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
            <span class="n">image_item</span> <span class="o">=</span> <span class="n">ImageItem</span><span class="p">()</span>
            <span class="n">image_item</span><span class="p">[</span><span class="s1">&#39;img_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
            <span class="k">yield</span> <span class="n">image_item</span>

        <span class="n">urls</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">url</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">next_url</span><span class="p">)</span>
</code></pre></div>

<p>我想了半天都不明白为什么，以为是过滤器的问题，查了半天资料仍没解决。 后来偶然看了 Spider 源码，才发现坑爹之处。</p>
<p>原来源码的 start_requests 是这样写的（已忽略无关代码）</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self …</span></code></pre></div> </div><!-- /.entry-content -->
                <div class="read-more">
                    <a class="read-more-link" href="/scrapy-start-requests-trap.html">>> Read More</a>
                </div>
        </article></li>
</ol><!-- /#posts-list -->
</section><!-- /#content -->

        <footer id="contentinfo" class="body mui--divider-top">
          <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh">
            <img id="cc" src="/images/cc.80x15.png" alt="CC BY-SA"></a>
                <address id="about" class="vcard body">
                Powered by <a href="https://github.com/getpelican/pelican">Pelican</a>.
                Theme <a href="/">Autism</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
  </div>

  <script defer src="/theme/js/image.js"></script>
</body>
</html>